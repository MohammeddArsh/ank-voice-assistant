# Ank â€” AI Voice Assistant

> A full-stack conversational AI assistant with a speech interface, built with Python and a clean dark web UI.
> Speak naturally, get intelligent responses spoken back to you in real time.

![Ank Demo](demo/demo.png)

ğŸ”— **[Live Demo](https://ank-voice-assistant.onrender.com)**

---

## Overview

Ank is a full-stack voice assistant that integrates speech recognition, large language model inference, and text-to-speech synthesis into a seamless conversational experience. Built to demonstrate end-to-end AI pipeline integration, from raw microphone audio to spoken AI responses, using production-grade APIs.

The system supports two modes â€” **OpenAI** (cloud, deployed) and **Local** (Ollama + faster-whisper, offline), switchable with a single config change. It also includes session logging and data export designed to support empirical human-AI interaction research.

**Live interaction flow:**

```
Your voice â†’ Whisper STT â†’ GPT-4o-mini â†’ gTTS â†’ Spoken response
```

---

## Features

- **Voice input** â€” Click the mic button or hold Spacebar to record
- **Real-time waveform** â€” Live symmetric audio visualizer while speaking
- **Conversational memory** â€” Remembers full session context across turns
- **Spoken responses** â€” Every reply converted to audio and played automatically
- **Chat history** â€” Full conversation displayed on screen with replay buttons
- **Session analytics** â€” Live stats: turn count, response times, session duration
- **Data export** â€” Download session data as JSON or CSV for research analysis
- **Consent banner** â€” GDPR-aware data usage notice on every session
- **Dual mode** â€” Switch between OpenAI APIs and fully local models
- **Mobile responsive** â€” Works on phones and tablets
- **New Chat** â€” Resets session, memory and analytics instantly

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Browser (UI)                     â”‚
â”‚  MediaRecorder API â†’ WAV blob â†’ fetch(/chat)        â”‚
â”‚  â† JSON { user_text, reply, audio_url, analytics }  â”‚
â”‚  Audio() playback â† /audio/{file}                   â”‚
â”‚  Export â† /export/json  /export/csv                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                FastAPI Backend                      â”‚
â”‚                                                     â”‚
â”‚  /chat                                              â”‚
â”‚    â”œâ”€â”€ audio/stt.py      (router â†’ openai or local) â”‚
â”‚    â”œâ”€â”€ brain/memory.py   (multi-turn history)       â”‚
â”‚    â”œâ”€â”€ brain/llm.py      (router â†’ openai or local) â”‚
â”‚    â”œâ”€â”€ brain/logger.py   (session logging)          â”‚
â”‚    â””â”€â”€ audio/tts.py      (gTTS â†’ MP3)               â”‚
â”‚                                                     â”‚
â”‚  /audio/{file}    serves generated MP3              â”‚
â”‚  /reset           clears memory and session         â”‚
â”‚  /export/json     downloads session as JSON         â”‚
â”‚  /export/csv      downloads session as CSV          â”‚
â”‚  /analytics       returns live session stats        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Component breakdown:**

| Component | File | Responsibility |
|---|---|---|
| Web UI | `static/index.html` | Recording, waveform, chat, analytics |
| API Server | `app.py` | Routing, session management, cleanup |
| STT Router | `audio/stt.py` | Routes to OpenAI or local Whisper |
| STT OpenAI | `audio/stt_openai.py` | Transcription via OpenAI Whisper API |
| STT Local | `audio/stt_local.py` | Transcription via faster-whisper locally |
| LLM Router | `brain/llm.py` | Routes to OpenAI or Ollama |
| LLM OpenAI | `brain/llm_openai.py` | GPT-4o-mini via OpenAI API |
| LLM Local | `brain/llm_local.py` | Llama 3.2 via Ollama locally |
| Memory | `brain/memory.py` | Multi-turn conversation history |
| Logger | `brain/logger.py` | Session logging, JSON/CSV export |
| TTS | `audio/tts.py` | Text-to-speech via gTTS + ffplay |

---

## Tech Stack

| Layer | Technology | Notes |
|---|---|---|
| Backend | Python 3.11, FastAPI, Uvicorn | Async REST API |
| Speech-to-Text | OpenAI Whisper API / faster-whisper | Cloud or local |
| LLM | GPT-4o-mini / Ollama + Llama 3.2 | Cloud or local |
| Text-to-Speech | gTTS + ffplay | Google TTS, free |
| Frontend | Vanilla HTML / CSS / JS | No framework, single file |
| Audio Capture | Web MediaRecorder API | Browser-native |
| Deployment | Render (Frankfurt EU) | Auto-deploy from GitHub |

---

## Setup & Installation

### Prerequisites

- Python 3.11+
- OpenAI API key â€” [platform.openai.com/api-keys](https://platform.openai.com/api-keys) *(OpenAI mode)*
- [Ollama](https://ollama.com) installed *(local mode only)*
- [ffmpeg](https://ffmpeg.org/download.html) installed

### 1. Clone the repository

```bash
git clone https://github.com/MohammeddArsh/ank-voice-assistant
cd ank-voice-assistant
```

### 2. Create a virtual environment

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Mac / Linux
source .venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure mode

Open `config.py` and set your preferred mode:

```python
MODE = "openai"   # "openai" = GPT-4o-mini + Whisper API
                  # "local"  = Llama 3.2 + faster-whisper (no API key needed)
```

### 5. Add your API key *(OpenAI mode only)*

Create a `.env` file in the project root:

```
OPENAI_API_KEY=sk-your-key-here
```

### 6. Configure ffmpeg *(local playback only)*

Open `audio/tts.py` and update the path:

```python
FFMPEG_DIR = r"C:\path\to\ffmpeg\bin"   # Windows
# FFMPEG_DIR = "/usr/local/bin"         # Mac / Linux
```

### 7. Pull the Llama model *(local mode only)*

```bash
ollama pull llama3.2
```

### 8. Run the server

```bash
uvicorn app:app --reload
```

### 9. Open in browser

```
http://localhost:8000
```

---

## Project Structure

```
ank-voice-assistant/
â”œâ”€â”€ app.py                  # FastAPI server, endpoints, temp file cleanup
â”œâ”€â”€ config.py               # Mode switch, API keys, model settings
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ render.yaml             # Render deployment config
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html          # Full web UI â€” single file
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ stt.py              # STT router
â”‚   â”œâ”€â”€ stt_openai.py       # Whisper API
â”‚   â”œâ”€â”€ stt_local.py        # faster-whisper (local)
â”‚   â””â”€â”€ tts.py              # Text-to-speech via gTTS
â””â”€â”€ brain/
    â”œâ”€â”€ llm.py              # LLM router
    â”œâ”€â”€ llm_openai.py       # GPT-4o-mini
    â”œâ”€â”€ llm_local.py        # Llama 3.2 via Ollama
    â”œâ”€â”€ memory.py           # Conversation history
    â””â”€â”€ logger.py           # Session logging and data export
```

---

## API Endpoints

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/` | Serves the web UI |
| `POST` | `/chat` | Accepts audio, returns transcript + reply + audio URL + analytics |
| `GET` | `/audio/{file}` | Serves generated TTS audio |
| `POST` | `/reset` | Clears memory, session and temp files |
| `GET` | `/analytics` | Returns live session analytics |
| `GET` | `/export/json` | Downloads session as JSON |
| `GET` | `/export/csv` | Downloads session as CSV |

---

## Research & Data Collection

Each session is automatically logged to `logs/session_YYYYMMDD_HHMMSS.json` containing full turn history, timestamps and response time metrics. Sessions can be exported directly from the UI as JSON or CSV for offline analysis.

This feature is designed to support empirical studies on human-AI dialogue interaction â€” tracking conversation patterns, response latency, and turn-taking behaviour across sessions.

---

## Privacy & Ethics

- A consent banner is shown at the start of every session explaining data usage
- Voice audio is sent to OpenAI Whisper API for transcription
- Conversation text is sent to GPT-4o-mini to generate responses
- No data is stored permanently on the server
- Session logs are saved locally and never shared with third parties
- The local mode option allows fully offline operation with zero data leaving the device

---

## Built by

**Mohammed Arsh** â€” [github.com/MohammeddArsh](https://github.com/MohammeddArsh)